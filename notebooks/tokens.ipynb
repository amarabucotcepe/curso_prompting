{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ffb08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstração de Tokens: Caracteres, Sílabas e Fonemas\n",
    "\n",
    "Neste notebook, vamos explorar o conceito de tokens e como eles diferem de outras unidades linguísticas como caracteres, sílabas e fonemas. Esta distinção é importante para entender como os modelos de linguagem processam o texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00e695ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98dc6cc",
   "metadata": {},
   "source": [
    "## 1. O que são Tokens?\n",
    "\n",
    "Tokens são as unidades básicas de processamento em modelos de linguagem. Ao contrário de caracteres ou palavras, os tokens representam fragmentos de texto que foram identificados por um processo chamado \"tokenização\". Dependendo do modelo e do tokenizador:\n",
    "\n",
    "- Um token pode ser menor que uma palavra (como um prefixo ou sufixo)\n",
    "- Um token pode ser uma palavra inteira\n",
    "- Um token pode ser maior que uma palavra (algumas frases comuns)\n",
    "- Caracteres especiais e espaços também podem ser tokens\n",
    "\n",
    "Vamos ver como o tokenizador do OpenAI GPT processa um texto simples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79c207e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original: Olá mundo! Como funciona a tokenização?\n",
      "Número de caracteres: 39\n",
      "Número de tokens: 10\n",
      "Tokens: [43819, 1995, 29452, 0, 46774, 87996, 264, 4037, 48476, 30]\n",
      "Tokens decodificados: ['Ol', 'á', ' mundo', '!', ' Como', ' funciona', ' a', ' token', 'ização', '?']\n"
     ]
    }
   ],
   "source": [
    "# Vamos usar o codificador cl100k_base, usado pelo GPT-4\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Exemplo de texto\n",
    "texto = \"Olá mundo! Como funciona a tokenização?\"\n",
    "\n",
    "# Tokenizar o texto\n",
    "tokens = encoding.encode(texto)\n",
    "\n",
    "# Decodificar cada token de volta para visualizar\n",
    "tokens_decodificados = [encoding.decode([token]) for token in tokens]\n",
    "\n",
    "print(f\"Texto original: {texto}\")\n",
    "print(f\"Número de caracteres: {len(texto)}\")\n",
    "print(f\"Número de tokens: {len(tokens)}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Tokens decodificados: {tokens_decodificados}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "156e19ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original: Pernambuco você é meu\n",
      "Caracteres: ['P', 'e', 'r', 'n', 'a', 'm', 'b', 'u', 'c', 'o', ' ', 'v', 'o', 'c', 'ê', ' ', 'é', ' ', 'm', 'e', 'u']\n",
      "Número de caracteres: 21\n",
      "Sílabas: ['Per', 'nam', 'bu', 'co', 'vo', 'cê', 'é', 'meu']\n",
      "Número de sílabas: 8\n",
      "Tokens: [47, 944, 3042, 95514, 25738, 4046, 56309]\n",
      "Tokens decodificados: ['P', 'ern', 'amb', 'uco', ' você', ' é', ' meu']\n",
      "Número de tokens: 7\n"
     ]
    }
   ],
   "source": [
    "texto = \"Pernambuco você é meu\"\n",
    "silabas = [\"Per\", \"nam\", \"bu\", \"co\", \"vo\", \"cê\", \"é\", \"meu\"]\n",
    "\n",
    "# Tokenizar o texto\n",
    "tokens = encoding.encode(texto)\n",
    "\n",
    "# Decodificar cada token de volta para visualizar\n",
    "tokens_decodificados = [encoding.decode([token]) for token in tokens]\n",
    "\n",
    "print(f\"Texto original: {texto}\")\n",
    "print(f\"Caracteres: {[c for c in texto]}\")\n",
    "print(f\"Número de caracteres: {len(texto)}\")\n",
    "print(f\"Sílabas: {silabas}\")\n",
    "print(f\"Número de sílabas: {len(silabas)}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Tokens decodificados: {tokens_decodificados}\")\n",
    "print(f\"Número de tokens: {len(tokens)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
